---
date: 2025-06-16T23:13:12+03:00
title: "Мой третий хакатон (отчёт по хакатону ogon.ai - MCP)"
draft: false
author: "Иван"
cover: ""
tags: ["ai", "hackathon", "mcp"]
keywords: ["хакатон", "MCP-сервер", "MCP-протокол"]
description: "Отчёт о хакатоне: когда докстринги в питоне влияют на поведение программы"
showFullContent: false
readingTime: true
draft: false
---

Сначала будет отчёт по хакатону, с описанием "как это было и чувствовалось". Во второй части — про инструменты которые я впервые потыкал.

## Отчёт

Изначально прочитал об идее этого онлайн-хакатон, и сразу подумал "круто, как раз про mcp, можно будет потыкать!". И когда понял, что собирается команда, решил присоединиться.

> Хакатон - это стартап в миниатюре.

Мы начали "хакатонить" в четверг, хотя можно было раньше, а закончили в понедельник, вечером, в 22:59.

У нас был продакт, 3 бэкендера (включая меня) и 1 фронтэндер. Мне очень понравилось, что на этом хакатоне у нас был продакт, с опытом хакатона в роли продакта, и что идея была вполне годная. Мне понравилось взаимодействие в команде. 

Организация работ выглядела как 2 созвона-"синка" в день, утром и вечером, со списком задач на "до следующего созвона". По некоторым штукам нужно было контактировать с другими программистами. Учёт фич и багов, а также взаимодействия с организатором хакатона — взял на себя продакт.

Вообще с самого начала думал, что идея будет хорошей, и реализация достаточно простой. Оказалось что так и есть, и приложение стало нам всерьёз нравиться. Да, есть что допиливать (во всех местах, наверное). Но...

> Добавление кардинально новой функциональности — это маленький pivot

У нас было два таких момента:

1. Когда мы решили делать бекенд на `n8n` (это было довольно хорошее решение; архитектура выглядит довольно симпатичной — мне кажется, поломки были бы неприятнее, если бы бек был на питоне).
2. Когда мы решили добавить на фронт и бек ещё одну функциональность, которая является, возможно, ключевой фичей.


### По дням

День первый — собрались, долго обсудили, я разработал MCP-сервер.
День второй, третий, четвёртый — у меня преимущественно n8n, также тестирование. В 3-4 день много сделал. Забавно, что засев за n8n пайплайн, довольно много переработал.
День пятый — доделали и подготовили к презентации.

---

### Что хорошо:

- я использовал в принципе многие свои навыки (в тч и те, которые слабые). Кажется, только фронтендом непосредственно не занимался
- я тестировал продукт, он получился прикольный (да, есть что доделать)
- я предлагал и обсуждал фичи
- я работал с бекендом на n8n (1 из 2 воркфлоу)
- я работал с бекендом на Python (MCP-сервер)
- я формулировал требования, если были нечёткие
- я собрал простенький пайплайн для сборки фронтенда, и `docker-compose.yml` для запуска моего MCP-сервера.

### Что можно было бы улучшить с моей стороны:

- не пропускать тестирование ключевых фич. Канбан доска с приоритетами, по идее, решает это, но в таких штуках есть обычно много зависимостей которые апдейтятся постоянно.
- иногда впадал в "клинч объяснения" в устной и "письменно-устной" (быстрый текст в чате). У меня часто бывают нечёткие формулировки, оговорки в терминологии, это мешает формулировать и мешает другим быстро меня понять. Стоит позаниматься своей речью, когда будут на это ресурсы.
- не затупливать над стереотипными проблемами. Кажется, это вообще что-то очень человеческое — когда одна и та же проблема, с вариациями, повторяется, но время на её исправление тратится почти столько же.

### Команда:

- всё офигенно!
- (мне оч понравилось, правда! спасибо отдельное продакту за организацию, это даже не выглядело хаосом (ну почти))
- возможно, была бы полезна какая-то доска/страница/сообщение с задачами и закреп с важными ссылками в тг-чате (в самом начале мы посидели над миро-доской, но вроде это было разово).

### Орги:

- понятные тексты/правила/требования. Впрочем, ещё не дождался объявления победителей)
- на вопросы по ходу дела отвечали
- консультацию с техническим специалистом дали, было содержательно
	- был момент, когда возникла как бы сама собой (но было подведено предыдущим диалогом) довольно очевидная, но хорошая идея
- каких-то ресурсов дополнительно выделено не было

## Новые (для меня) инструменты

### n8n.io

`n8n` - low-code инструмент.

- Плюсы: довольно простой, есть много разных нод, включая ИИ-шные.
- Эстетичный, логичный интерфейс. 
- Опенсорс (важный плюс), хотя есть и премиум версии.
- Есть сообщество, да и документация
- Минусы:
	- селф-хостед версия 
		- не содержит общих воркфлоу, в отличие от облачной. Можно редактировать что угодно из-под админского аккаунта, можно всем вместе сидеть из-под одного аккаунта. 
		- не содержит (кажется) отладки по ходу дела, в отличие от облачной.
	- местами очень неприятные баги (ладно, не самые неприятные, но неприятные):
		- один раз я хотел обновить выходной формат ноды. Это была иишная нода, у неё была [доп. нода Structured Output](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/#related-resources), чтобы она проверяла формат. Я изменил JSON-схему в ноде, но почему-то вывод выводился то в старом формате, то в новом. По просмотру промта стало понятно, что схема подтягивается старая. Понадобилось, кажется, заново создать Structured Output ноду.
		- не удавалось завершить по ошибке запущенный, очень долгий воркфлоу. Нода LLM-ки работала при всех возможных попытках её остановить, выдавала ошибки из-за превышений rate limit. При отключении от основной ноды (агента), она переставала, но при обратном подключении начинала снова. Лечилось аналогично — созданием новой ноды xD
		- мелочь, но: есть нода LLM summary, у неё есть режим пересказа по чанкам (старая идея: т.к. текст может быть очень большим, то мы сначала перескажем чанки с пересечением, а потом перескажем все эти пересказы). Если текст полностью влезает в 1 чанк — запроса будет всё равно 2, и это вроде не исправить... Пришлось избавиться от этой ноды.
		- в облачной версии мало ресурсов (сообщают что 300 МБ) и нет поддержки axios/asyncio по умолчанию. На 1ГБ сервере селф-хостед версия грузила почти на 100% RAM и процессор.

### Cursor

`Cursor` - AI-powered редактор кода.

- Плюсы: ИИ, опенсорс (кроме ИИ-моделей)
- Минусы для меня: VS Code (что непривычно), модели не локальные. Стоит денег))

### MCP-протокол

MCP — протокол, позволяющий LLM-модели взаимодействовать с внешними инструментами, когда это нужно.

- Плюсы: 
	- минимальный MCP пишется очень быстро. Я тыкал курсор и MCP некоторое время, а потом стал натыкаться на видео типа "Я написал MCP-сервер за 20 минут". so-so
	- выполняет мечту "дать LLM все инструменты а она уже сама". 
- Особенности:
	- учитывается как промт LLM-агента, так и промт MCP-сервера (описание предоставляемого им API, по сути). В той библиотеке которую использовал я - докстринг функции-MCP-tool являлся её описанием для бека.
		- при некоторых проблемах
	- есть 3 способа коммуникации, только 2 из них сетевые.
	- немного сбивающая терминология — в целом, речь обычно идёт об MCP-клиентах и MCP-серверах, при этом MCP-клиентом могут называть как и программу, использующую сервера (Cursor, Copilot, кастомный LLM-агент...), так и непосредственно подключение к MCP-серверу.
	- ГОВОРЯТ, есть особенности с безопасностью (классической).
	- очевидно, есть особенности с безопасностью (которая AI Safety, промт-инъекции): если не контролировать доступ к MCP-server никак, то могут возникнуть проблемы. Есть MCP-сервера для баз данных, например, которые делают к ним запросы.

## System Prompt Generator (чатбот у ChatGPT)

Полезная утилита, позволяющая написать хороший промт (по сути ТЗ) для языковой модели. Выглядит как странный текст, переполненный псевдоMarkdown форматированием и капсом, но работает же. Под конец сам уловил паттерн написания такого и теперь могу писать в таком же стиле.

`Особенность`: Несколько раз замечал, что чатботы работают лучше, когда в самом начале посылается полное ТЗ на желаемый код, а не происходят попытки дополнить во многих сообщениях. С этим чатботом тоже так.