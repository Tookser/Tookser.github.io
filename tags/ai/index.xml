<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ai on Личный сайт Ивана Белашкина</title><link>https://tookser.github.io/tags/ai/</link><description>Recent content in ai on Личный сайт Ивана Белашкина</description><generator>Hugo -- gohugo.io</generator><language>ru-ru</language><lastBuildDate>Sun, 10 Mar 2024 20:36:38 +0300</lastBuildDate><atom:link href="https://tookser.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Промт-инъекция, видео Карпатого и пара быстрых моделей</title><link>https://tookser.github.io/posts/neural1/</link><pubDate>Sun, 10 Mar 2024 20:36:38 +0300</pubDate><guid>https://tookser.github.io/posts/neural1/</guid><description>Начал проходить курс &amp;ldquo;AI Safety fundamentals&amp;rdquo;, и там есть видео Андрея Карпатого про LLM. Больше половины инфы оттуда я знал, но:
идея про то, что языковые модели как класс программ ведут себя аналогично операционным системам (LLM OS). Подробности по ссылке (видео на английском). там очень классное описание возможных атак на LLM (промт-инъекций и не только), со ссылками на статьи. Понятно, что это всё прикрывается достаточно быстро. Jailbroken: How Does LLM Safety Training Fail?</description><content>&lt;p>Начал проходить курс &amp;ldquo;AI Safety fundamentals&amp;rdquo;, и там есть &lt;a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">видео Андрея Карпатого про LLM&lt;/a>. Больше половины инфы оттуда я знал, но:&lt;/p>
&lt;ul>
&lt;li>идея про то, что языковые модели как класс программ ведут себя аналогично операционным системам (&lt;em>LLM OS&lt;/em>). Подробности по &lt;a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;amp;t=2535s">ссылке&lt;/a> (видео на английском).&lt;/li>
&lt;li>там очень классное описание возможных атак на LLM (промт-инъекций и не только), со ссылками на статьи. Понятно, что это всё прикрывается достаточно быстро.
&lt;ul>
&lt;li>&lt;a href="https://arxiv.org/abs/2307.02483">Jailbroken: How Does LLM Safety Training Fail?&lt;/a>
&lt;ul>
&lt;li>оттуда, например - про base64 кодирование как метод обхода. LLM умеют читать base64, но не защищены.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ещё пара статей про отдельные виды атак &lt;a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;amp;t=2743s">(ссылка с таймкодом на последнюю часть видео про это)&lt;/a>, в том числе на мультимодальные модели.&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models&lt;/a> - про универсальный, выглядящий набором мусора промт-суффикс. Это надо смотреть.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="пара-ныне-недоступных-быстрых-моделей">Пара (ныне недоступных) быстрых моделей&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.ixbt.com/news/2024/02/20/nikomu-neizvestnyj-startap-groq-predstavil-vidimo-luchshij-processor-dlja-vyvoda-nejrosetevyh-modelej.html">Groq (ixbt)&lt;/a> - ASIC (специализированная микросхема) для быстрого инференса LLM. Потыкать можно &lt;a href="https://groq.com">тут,&lt;/a> (там Llama, Mistral, есть платное АПИ), это впечатляет скоростью. В какой-то момент без прокси перестало быть доступным.&lt;/li>
&lt;li>&lt;a href="https://fastsdxl.ai">https://fastsdxl.ai&lt;/a> - это быстрая генерация картинок, со скоростью ввода. Скорость достигнута в т.ч. благодаря использованию маленькой модели SD XL Lightning. Сейчас недоступна, &lt;strong>обещают вернуться.&lt;/strong>&lt;/li>
&lt;/ul></content></item><item><title>Машинное обучение, ссылки (1)</title><link>https://tookser.github.io/posts/ml_1/</link><pubDate>Sat, 18 Mar 2023 03:21:05 +0300</pubDate><guid>https://tookser.github.io/posts/ml_1/</guid><description>Немного книг, ссылок и сервисов про ML.
Николенко, Введение в глубокое обучение. Написано местами многословно, но увлекательно, с другой стороны - сложные вещи требуют дополнительного гуглинга. Тензорфлоу там используется согласно сети старый, поэтому на него внимательно не смотрел. https://www.deeplearningbook.org/ (англ.) от Goodfellow с соавторами. Пока не смотрел, возможно также, не скачивается в удобном виде. Лекции и семинары Евгения Соколова (ВШЭ). Классика (15 часов) и глубокое обучение (72*1.5 часа), рекомендовались как хорошие видео с понятными объяснениями.</description><content>&lt;p>Немного книг, ссылок и сервисов про ML.&lt;/p>
&lt;ol>
&lt;li>Николенко, Введение в глубокое обучение. Написано местами многословно, но увлекательно, с другой стороны - сложные вещи требуют дополнительного гуглинга. Тензорфлоу там используется согласно сети старый, поэтому на него внимательно не смотрел.&lt;/li>
&lt;li>&lt;a href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/&lt;/a> (англ.) от Goodfellow с соавторами. Пока не смотрел, возможно также, не скачивается в удобном виде.&lt;/li>
&lt;li>Лекции и семинары Евгения Соколова (ВШЭ). &lt;a href="https://www.youtube.com/playlist?list=PLEqoHzpnmTfChItexxg2ZfxCsm-8QPsdS">Классика (15 часов)&lt;/a> и &lt;a href="https://www.youtube.com/playlist?list=PLEwK9wdS5g0qa3PIhR6HBDJD_QnrfP8Ei">глубокое обучение (72*1.5 часа)&lt;/a>, рекомендовались как хорошие видео с понятными объяснениями.&lt;/li>
&lt;/ol>
&lt;p>Мелочи.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://zhang-yang.medium.com/how-pytorch-tensors-backward-accumulates-gradient-8d1bf675579b">Статья на медиуме&lt;/a> про autograd в pytorch. См. также немного &lt;a href="https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch">на SO&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://open-assistant.io/ru">https://open-assistant.io/ru&lt;/a>, было бы здорово к этому движу присоединиться.&lt;/li>
&lt;li>Боты по типу ChatGPT, но в ТГ:
&lt;ul>
&lt;li>&lt;a href="https://t.me/talkgptbot">https://t.me/talkgptbot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://t.me/gpt3_unlim_chatbot">https://t.me/gpt3_unlim_chatbot&lt;/a> (не unlim в бесплатной версии)&lt;/li>
&lt;li>&lt;a href="https://t.me/chatGPTwrapperbot">https://t.me/chatGPTwrapperbot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://t.me/chatgpt_karfly_bot">https://t.me/chatgpt_karfly_bot&lt;/a>&lt;/li>
&lt;li>особняком стоит &lt;a href="https://t.me/VoiceLogBot">https://t.me/VoiceLogBot&lt;/a> - у этого есть конкретное назначение (быть чем-то вроде коуча-дневника, задающим наводящие вопросы на подумать), плюс он принимает голосовые и кружочки (не проверял качество), &lt;del>и безлимитный&lt;/del> небезлимитный(. Чувствую, эта область (psy-боты в ТГ) ещё минимум год будет развиваться.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://huggingface.co/spaces/huggingface/diffuse-the-rest">https://huggingface.co/spaces/huggingface/diffuse-the-rest&lt;/a> inpainting (дорисовывание картинки/скетча), требует также текстовый промт. Иногда выдаёт неинтересное совсем, иногда результат далёк от наброска.&lt;/li>
&lt;/ul></content></item></channel></rss>